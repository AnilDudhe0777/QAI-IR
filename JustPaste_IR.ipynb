{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5fe8b515-344b-436a-91da-d544fec1a8b7",
   "metadata": {},
   "source": [
    "## Q-6 Stopword Steamming\n",
    "Write a program for pre-processing of a text document such as stop word removal, stemming."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f5c8840-4ef9-4a46-be05-792c328dd2a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "\n",
    "print(stopwords.words('english'))\n",
    "\n",
    "sent = \"This is a sample sentence, showing off the stop words filtration.\"\n",
    "stop_words = set(stopwords.words('english'))\n",
    "word_tokens = word_tokenize(sent)\n",
    "\n",
    "# converts the words in word_tokens to lower case and then checks whether they are present in stop_words or not\n",
    "filtered_sent = [w for w in word_tokens if not w.lower() in stop_words]\n",
    "filtered_sent = []\n",
    "for w in word_tokens:\n",
    " if w not in stop_words:\n",
    "     filtered_sent.append(w)\n",
    "print(word_tokens)\n",
    "print(filtered_sent)\n",
    "\n",
    "f1 = open(\"input.txt\")\n",
    "line = f1.read()\n",
    "words = line.split()\n",
    "print(\"Before Removing Stopwords:\",words)\n",
    "print(\"Length:\",len(words))\n",
    "\n",
    "for r in words:\n",
    "    if not r in stop_words:\n",
    "        appendfile = open('output.txt','a')\n",
    "        appendfile.write(\" \"+r)\n",
    "        appendfile.close()\n",
    "\n",
    "f2 = open(\"output.txt\")\n",
    "line2 = f2.read()\n",
    "words = line2.split()\n",
    "print(\"Before Removing Stopwords:\",words)\n",
    "print(\"Length:\",len(words))\n",
    "\n",
    "ps = PorterStemmer()\n",
    "\n",
    "sample = ['program', 'programmer', 'programming', 'programs', 'programmers']\n",
    "for s in sample:\n",
    "    print(s,\":\", ps.stem(s))\n",
    "\n",
    "sent = \"Programmers program with different programming languages\"\n",
    "words = word_tokenize(sent)\n",
    "print(words)\n",
    "\n",
    "for w in words:\n",
    "    print(w,\":\", ps.stem(w))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33117d9f-d7a1-45fb-9e67-fa3d21c4e678",
   "metadata": {},
   "source": [
    "## Q-7 Inverted File\n",
    "Implement a program for retrieval of documents using inverted files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48732e83-b6b0-4ae1-904e-9bee285c0be9",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc1 = \"The quick brown fox jumped over the lazy dog.\"\n",
    "doc2 = \"The lazy dog slept in the sun.\"\n",
    "\n",
    "# Convert each document to lowercase and split it into words\n",
    "token1 = doc1.lower().split()\n",
    "token2 = doc2.lower().split()\n",
    "print(\"Token1:\",token1)\n",
    "print(\"Token2 :\",token2)\n",
    "\n",
    "# Combine the tokens into a list of unique terms\n",
    "terms = list(set(token1 + token2))\n",
    "print(\"Terms :\",terms)\n",
    "\n",
    "# Create an empty dictionary to store the inverted index\n",
    "inverted_index = {}\n",
    "# For each term, find the documents that contain it\n",
    "for term in terms:\n",
    "    doc = []\n",
    "    if term in token1:\n",
    "        doc.append(\"Doc1\")\n",
    "    if term in token2:\n",
    "        doc.append(\"Doc2\")\n",
    "    inverted_index[term] = doc\n",
    "print(\"Inverted Index Dictionary :\", inverted_index)\n",
    "\n",
    "# Print inverted index\n",
    "for term, doc in inverted_index.items():\n",
    "    print(\"term ->\", ','.join(doc))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c3fa67c-8b89-4f88-8769-18becd8edd42",
   "metadata": {},
   "source": [
    "## Q-8 Bayesian Network\n",
    "Write a program to construct a Bayesian network considering medical data. Use this model to\n",
    "demonstrate the diagnosis of heart patients using the standard Heart Disease Data Set (You can use\n",
    "Java/Python ML library classes/API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adc9db63-86ff-4cc7-88fa-69f5ca7afe0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pgmpy==0.1.16\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pgmpy.models import BayesianNetwork\n",
    "from pgmpy.estimators import MaximumLikelihoodEstimator\n",
    "from pgmpy.inference import VariableElimination\n",
    "\n",
    "df= pd.read_csv('heart.csv')\n",
    "df.head()\n",
    "\n",
    "#In dataset has some value as '?' so removed it\n",
    "df=df.replace('?', np.nan) \n",
    "\n",
    "# Model Bayesian Network\n",
    "model = BayesianNetwork([('age','trestbps'),('age','fbs'),('sex','trestbps'),('exang','trestbps'),('trestbps','heartdisease'),\n",
    "                       ('fbs','heartdisease'),('heartdisease','restecg'), ('heartdisease','thalach'),('heartdisease','chol')])\n",
    "\n",
    "# Learning CPDs using Maximum Likelihood Estimators\n",
    "print(\"Learning CPD using Maximum likelihood estimators\")\n",
    "model.fit(df, estimator=MaximumLikelihoodEstimator)\n",
    "\n",
    "# Inferencing with Bayesian Network\n",
    "print('Inferencing with Bayesian Network:')\n",
    "HeartDisease_infer = VariableElimination(model)\n",
    "\n",
    "# Computing the Probability of HeartDisease given Age\n",
    "print(\"1. Probability of HeartDisease given Age=35\")\n",
    "p= HeartDisease_infer.query(variables=['heartdisease'], evidence={'age':35})\n",
    "print(p)\n",
    "\n",
    "# Computing the Probability of HeartDisease given cholesterol\n",
    "print(\"2. Probability of HeartDisease given cholesterol=230\")\n",
    "p=HeartDisease_infer.query(variables=['heartdisease'], evidence ={'chol':230})\n",
    "print(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41b50c2e-fdc2-4dfd-822d-56b1fe2c761b",
   "metadata": {},
   "source": [
    "## Q-9 Agglomerative clustering\n",
    "Implement Agglomerative hierarchical clustering algorithm using appropriate dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3e2b7c1-a4c1-4f8b-bc62-09afaff3774f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from sklearn.preprocessing import StandardScaler, normalize\n",
    "from sklearn.metrics import silhouette_score\n",
    "import scipy.cluster.hierarchy as shc\n",
    "\n",
    "df = pd.read_csv('CC_GENERAL.csv')\n",
    "df.head()\n",
    "\n",
    "df.info()\n",
    "\n",
    "# Dropping the CUST_ID column from the data\n",
    "df =df.drop('CUST_ID', axis = 1)\n",
    "\n",
    "df.isnull().sum()\n",
    "\n",
    "# Handling the missing values\n",
    "df['CREDIT_LIMIT'].fillna(df['CREDIT_LIMIT'].mean(), inplace=True)\n",
    "df['MINIMUM_PAYMENTS'].fillna(df['MINIMUM_PAYMENTS'].mode()[0], inplace = True)\n",
    "\n",
    "df.isnull().sum()\n",
    "\n",
    "# Scaling the data so that all the features become comparable\n",
    "sc = StandardScaler()\n",
    "df_scaled = sc.fit_transform(df)\n",
    "\n",
    "# Normalizing the data so that the data approximately follows a Gaussian distribution\n",
    "df_normalized = normalize(df_scaled)\n",
    "\n",
    "# Converting the numpy array into a pandas\n",
    "data_normalized = pd.DataFrame(df_normalized)\n",
    "\n",
    "# Reducing the dimensionality of the Data\n",
    "pca = PCA(n_components = 2)\n",
    "df_principal = pca.fit_transform(df_normalized)\n",
    "df_principal = pd.DataFrame(df_principal)\n",
    "df_principal.columns = ['P1', 'P2']\n",
    "\n",
    "# Visualizing theworking of the Dendrograms\n",
    "# Dendrograms are used to divide a given clusterinto many different clusters\n",
    "plt.figure(figsize =(8, 8))\n",
    "plt.title('Visualising the data')\n",
    "Dendrogram = shc.dendrogram((shc.linkage(df_principal, method ='ward')))\n",
    "\n",
    "# Building and Visualizing the different clustering models for different values of k\n",
    "# k = 2\n",
    "ac2 = AgglomerativeClustering(n_clusters = 2)\n",
    "plt.figure(figsize =(6, 6))\n",
    "plt.scatter(df_principal['P1'], df_principal['P2'],\n",
    "c = ac2.fit_predict(df_principal))\n",
    "plt.show()\n",
    "\n",
    "# k = 3\n",
    "ac3 = AgglomerativeClustering(n_clusters = 3)\n",
    "plt.figure(figsize =(6, 6))\n",
    "plt.scatter(df_principal['P1'],df_principal['P2'],\n",
    "c = ac3.fit_predict(df_principal))\n",
    "plt.show()\n",
    "\n",
    "# k = 4\n",
    "ac4 = AgglomerativeClustering(n_clusters = 4)\n",
    "plt.figure(figsize =(6, 6))\n",
    "plt.scatter(df_principal['P1'],df_principal['P2'],\n",
    "c = ac4.fit_predict(df_principal))\n",
    "plt.show()\n",
    "\n",
    "# k = 5\n",
    "ac5 = AgglomerativeClustering(n_clusters = 5)\n",
    "plt.figure(figsize =(6, 6))\n",
    "plt.scatter(df_principal['P1'],df_principal['P2'],\n",
    "c = ac5.fit_predict(df_principal))\n",
    "plt.show()\n",
    "\n",
    "# k = 6\n",
    "ac6 = AgglomerativeClustering(n_clusters = 6)\n",
    "plt.figure(figsize =(6, 6))\n",
    "plt.scatter(df_principal['P1'],df_principal['P2'],\n",
    "c = ac6.fit_predict(df_principal))\n",
    "plt.show()\n",
    "\n",
    "# Evaluating the different models and Visualizing the results.\n",
    "k = [2, 3, 4, 5, 6]\n",
    "\n",
    "# Appending the silhouette scores of the different models to the list\n",
    "silhouette_scores = []\n",
    "silhouette_scores.append(silhouette_score(df_principal, ac2.fit_predict(df_principal)))\n",
    "silhouette_scores.append(silhouette_score(df_principal, ac3.fit_predict(df_principal)))\n",
    "silhouette_scores.append(silhouette_score(df_principal, ac4.fit_predict(df_principal)))\n",
    "silhouette_scores.append(silhouette_score(df_principal, ac5.fit_predict(df_principal)))\n",
    "silhouette_scores.append(silhouette_score(df_principal, ac6.fit_predict(df_principal)))\n",
    "print(silhouette_scores)\n",
    "\n",
    "\n",
    "# Plotting a bar graph to compare the results\n",
    "plt.bar(k,silhouette_scores)\n",
    "plt.xlabel('Number of Clusters')\n",
    "plt.ylabel('S(i)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58b20188-bb7c-4fa1-98fd-425584612418",
   "metadata": {},
   "source": [
    "## Q-10 Page Rank\n",
    "Implement Page Rank Algorithm. (Use python or beautiful soup for implementation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fed26150-9549-4080-9db2-d039d95909a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pagerank(G, alpha=0.85, personalization=None, max_iter=100, tol=1.0e-6, nstart=None, weight='weight', dangling=None):\n",
    "    if len(G) == 0:\n",
    "        return {}\n",
    "    if not G.is_directed():\n",
    "        D = G.to_directed()\n",
    "    else:\n",
    "        D = G\n",
    "\n",
    "    # Create a copy in (right) stochastic form\n",
    "    W = nx.stochastic_graph(D, weight=weight)\n",
    "    N = W.number_of_nodes()\n",
    "\n",
    "    # Choose fixed starting vector if not given\n",
    "    if nstart is None:\n",
    "        x = dict.fromkeys(W, 1.0 / N)\n",
    "    else:\n",
    "        s = float(sum(nstart.values())) # Normalized nstart vector\n",
    "        x = dict((k, v / s) for k, v in nstart.items())\n",
    "\n",
    "    if personalization is None:\n",
    "        p = dict.fromkeys(W, 1.0 / N) # Assign uniform personalization vector if not given\n",
    "    else:\n",
    "        missing = set(G) - set(personalization)\n",
    "        if missing:\n",
    "            raise NetworkXError('Personalization dictionary must have a value for every node. Missing nodes %s' % missing)\n",
    "        s = float(sum(personalization.values()))\n",
    "        p = dict((k, v / s) for k, v in personalization.items())\n",
    "\n",
    "    if dangling is None:\n",
    "        dangling_weights = p# Use personalization vector if dangling vector not specified\n",
    "    else:\n",
    "        missing = set(G) - set(dangling)\n",
    "        if missing:\n",
    "            raise NetworkXError('Dangling node dictionary must have a value for every node. Missing nodes %s' % missing)\n",
    "        s = float(sum(dangling.values()))\n",
    "        dangling_weights = dict((k, v/s) for k, v in dangling.items())\n",
    "\n",
    "    dangling_nodes = [n for n in W if W.out_degree(n, weight=weight) == 0.0]\n",
    "\n",
    "    # power iteration: make up to max_iter iterations\n",
    "    for _ in range(max_iter):\n",
    "        xlast = x\n",
    "        x = dict.fromkeys(xlast.keys(), 0)\n",
    "        danglesum = alpha * sum(xlast[n] for n in dangling_nodes)\n",
    "        for n in x:\n",
    "            # this matrix multiply looks odd because it is doing a left multiply x^T=xlast^T*W\n",
    "            for nbr in W[n]:\n",
    "                x[nbr] += alpha * xlast[n] * W[n][nbr][weight]\n",
    "            x[n] += danglesum * dangling_weights[n] + (1.0 - alpha) * p[n]\n",
    "\n",
    "        # check convergence, l1 norm\n",
    "        err = sum([abs(x[n] - xlast[n]) for n in x])\n",
    "        if err < N*tol:\n",
    "            return x\n",
    "    raise NetworkXError('Pagerank: power iteration failed to converge in %d iterations.' % max_iter)\n",
    "\n",
    "\n",
    "\n",
    "import networkx as nx\n",
    "G = nx.barabasi_albert_graph(60, 41)\n",
    "pr = nx.pagerank(G, 0.4)\n",
    "print(pr)\n",
    "\n",
    "\n",
    "ANOTHER ONE\n",
    "\n",
    "\n",
    "import networkx as nx\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "\n",
    "\n",
    "class PageRank:\n",
    "    def __init__(self, damping_factor=0.85, max_iter=100, tol=1.0e-6):\n",
    "        self.damping_factor = damping_factor\n",
    "        self.max_iter = max_iter\n",
    "        self.tol = tol\n",
    "        self.graph = nx.DiGraph()\n",
    "\n",
    "    def add_edge(self, from_node, to_node):\n",
    "        self.graph.add_edge(from_node, to_node)\n",
    "\n",
    "    def compute_pagerank(self):\n",
    "        pagerank = nx.pagerank(self.graph, alpha=self.damping_factor, max_iter=self.max_iter, tol=self.tol)\n",
    "        return pagerank\n",
    "\n",
    "    def scrape_links(self, url):\n",
    "        try:\n",
    "            response = requests.get(url)\n",
    "            soup = BeautifulSoup(response.text, 'html.parser')\n",
    "            links = set()\n",
    "            for link in soup.find_all('a', href=True):\n",
    "                links.add(link['href'])\n",
    "            return links\n",
    "        except Exception as e:\n",
    "            print(f\"Error scraping {url}: {e}\")\n",
    "            return set()\n",
    "\n",
    "    def build_graph_from_urls(self, seed_url, depth=1):\n",
    "        urls_to_visit = [seed_url]\n",
    "        visited_urls = set()\n",
    "\n",
    "        for _ in range(depth):\n",
    "            new_urls = []\n",
    "            for url in urls_to_visit:\n",
    "                if url not in visited_urls:\n",
    "                    visited_urls.add(url)\n",
    "                    links = self.scrape_links(url)\n",
    "                    for link in links:\n",
    "                        self.add_edge(url, link)  # Create edges in the graph\n",
    "                        new_urls.append(link)\n",
    "            urls_to_visit = new_urls\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "pagerank = PageRank()\n",
    "# Example: Build graph from URLs (seeds)\n",
    "# https://en.wikipedia.org/wiki/Web_scraping\n",
    "# https://medium.com/@arti.singh280/list/the-quantum-world-6126d55e1882\n",
    "seed_url = \"https://docs.quantum.ibm.com/api/qiskit/release-notes/0.44#misc-deprecations\"  # Change this URL as needed\n",
    "pagerank.build_graph_from_urls(seed_url, depth=2)\n",
    "\n",
    "# Compute PageRank\n",
    "ranks = pagerank.compute_pagerank()\n",
    "\n",
    "# Print the PageRank scores\n",
    "print(\"PageRank Scores:\")\n",
    "for url, score in ranks.items():\n",
    "    print(f\"{url}: {score:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
